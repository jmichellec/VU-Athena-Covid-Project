{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "simplified-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pattern.nl import sentiment\n",
    "import numpy as np\n",
    "\n",
    "from plotnine import ggplot, aes, geom_vline, geom_histogram\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "advised-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(csv_file):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    csv_file: file with 'message' as column name for text\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file, delimiter=';').fillna('None')\n",
    "    \n",
    "    # Remove rows where messages are empty or character length smaller than 10\n",
    "    df = df[(df['message'] != 'None') | (df['message'].apply(lambda x: len(x) >= 10))]\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates(subset='message')  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "incident-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preprocessing(df, platform):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    df: dataframe\n",
    "    platform: type of platform (facebook, twitter, etc.)\n",
    "    \"\"\"\n",
    "    if(platform == 'facebook'):\n",
    "        \n",
    "        # Rename columns\n",
    "        df = df.rename(columns={'like.summary.total_count': 'like_count', \n",
    "                  'love.summary.total_count': 'love_count',\n",
    "                  'haha.summary.total_count': 'haha_count',\n",
    "                  'wow.summary.total_count': 'wow_count',\n",
    "                  'sad.summary.total_count': 'sad_count',\n",
    "                  'angry.summary.total_count': 'angry_count'\n",
    "                  })\n",
    "        \n",
    "        # Reformat date\n",
    "        df['query_time'] = df['query_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f').date().strftime('%Y-%m-%d') if x != 'None' else 'None')\n",
    "        df['created_time'] = df['created_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%z').date().strftime('%Y-%m-%d') if x != 'None' else 'None')\n",
    "        \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "activated-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(df):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    df: dataframe\n",
    "    \"\"\"\n",
    "    # Each word in the lexicon has scores for:\n",
    "    # 1)     polarity: negative vs. positive    (-1.0 => +1.0)\n",
    "    # 2) subjectivity: objective vs. subjective (+0.0 => +1.0)\n",
    "    # 3)    intensity: modifies next word?      (x0.5 => x2.0)\n",
    "    \n",
    "    # Add columns sentiment and subjectivity\n",
    "    df['sentiment'] = df.apply(lambda x: sentiment(x.message)[0], 1)\n",
    "    df['subjectivity'] = df.apply(lambda x: sentiment(x.message)[1], 1) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "satellite-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_stats(df):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    df: dataframe\n",
    "    \"\"\"\n",
    "    # Means of sentiment and subjectivity\n",
    "    sent_mean = df['sentiment'].mean()\n",
    "    subj_mean = df['subjectivity'].mean()\n",
    "    \n",
    "    print(\"Sentiment mean:\", sent_mean)\n",
    "    print(\"Subjectivity mean\", subj_mean)\n",
    "    \n",
    "    # Plot sentiment scores\n",
    "    plt_sent = ggplot(df, aes(x='sentiment')) + geom_histogram(bins = 30, color = 'black', fill = 'gray') + geom_vline(\n",
    "                aes(xintercept=sent_mean), \n",
    "                linetype='dashed', size=0.6)\n",
    "    print(plt_sent)\n",
    "    \n",
    "    plt_subj = ggplot(df, aes(x='subjectivity')) + geom_histogram(bins = 30, color = 'black', fill = 'gray') + geom_vline(\n",
    "                aes(xintercept=subj_mean), \n",
    "                linetype='dashed', size=0.6)\n",
    "    print(plt_subj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "mighty-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove urls\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # Remove punctuation and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'[0-9]', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolower = TRUE, \n",
    "remove_numbers = TRUE, \n",
    "remove_punct = TRUE, \n",
    "remove_url = TRUE, \n",
    "remove_symbols = TRUE,\n",
    "remove = c(stop_vec,emoji_vec, stopwords('dutch'),stopwords('english')),\n",
    "stem = TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nl = nltk.corpus.stopwords.words('dutch')\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "more extensive list of dutch stopwords:\n",
    "mystopwords <- read.table(\"stop_words_dutch.txt\", header = TRUE)\n",
    "class(mystopwords)\n",
    "stop_vec = as.vector(mystopwords$Custom_stopwords)\n",
    "class(stop_vec)\n",
    "#Also drop basic English stopwords\n",
    "stopwords('english')\n",
    "#List of emojis & symbols\n",
    "emoji <- read.table(\"stop_words_emojis.txt\", header=TRUE)\n",
    "emoji_vec = as.vector(emoji$Emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "confirmed-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfm(df):\n",
    "    stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "\n",
    "    cv = CountVectorizer(stop_words=frozenset(lijst))\n",
    "    X = cv.fit_transform(df['message'].values)\n",
    "    result = pd.DataFrame(data=X.toarray(), columns=cv.get_feature_names())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['He is a good person',\n",
    "          'He is bad student',\n",
    "          'He is hardworking']\n",
    "df = pd.DataFrame(data=corpus, columns=['sentences'])\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=['he', 'is', 'a', 'good', 'person', 'bad', 'student', 'hardworking'], min_df=0,\n",
    "                             stop_words=frozenset(), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = vectorizer.fit_transform(df['sentences'].values)\n",
    "result = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a text corpus\n",
    "quantedacorpus <- corpus(fb,\n",
    "                         text_field = \"message\",\n",
    "                         meta = list(\"id\", \"created_time\", \"comment_count\",\"Like_count\" ),\n",
    "                         unique_docnames = TRUE)\n",
    "#Prepare data\n",
    "#Stopwords\n",
    "stopwords('dutch')\n",
    "#more extensive list of dutch stopwords:\n",
    "mystopwords <- read.table(\"stop_words_dutch.txt\", header = TRUE)\n",
    "class(mystopwords)\n",
    "stop_vec = as.vector(mystopwords$Custom_stopwords)\n",
    "class(stop_vec)\n",
    "#Also drop basic English stopwords\n",
    "stopwords('english')\n",
    "#List of emojis & symbols\n",
    "emoji <- read.table(\"stop_words_emojis.txt\", header=TRUE)\n",
    "emoji_vec = as.vector(emoji$Emojis)\n",
    "\n",
    "\n",
    "# Create document-frequency-matrix for topic model\n",
    "dfm_fb <- dfm(quantedacorpus, \n",
    "                  tolower = TRUE, \n",
    "                  remove_numbers = TRUE, \n",
    "                  remove_punct = TRUE, \n",
    "                  remove_url = TRUE, \n",
    "                  remove_symbols = TRUE,\n",
    "                  remove = c(stop_vec,emoji_vec, stopwords('dutch'),stopwords('english')),\n",
    "                  stem = TRUE)\n",
    "\n",
    "######Explore\n",
    "#Get most frequent terms\n",
    "topfeatures(\n",
    "  dfm_fb,\n",
    "  n = 200,\n",
    "  decreasing = TRUE,\n",
    "  scheme = c(\"count\", \"docfreq\"),\n",
    "  groups = NULL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "oriental-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"test_file\", help = \"Path to feature file of data classifier is tried on\", type = str)\n",
    "#     parser.add_argument(\"output_path\", help = \"Path to folder whit the output of the classifier on the data\", type = str)\n",
    "    \n",
    "#     args = parser.parse_args(['--file', '/path/to/sequences.txt'])\n",
    "    csv_file = 'FB_NOS_NU_Telegraaf_NRC_all_endFeb.csv'\n",
    "    platform = 'facebook'\n",
    "    df = read_file(csv_file)\n",
    "    df = df_preprocessing(df, platform)\n",
    "#     df = calculate_sentiment(df)\n",
    "#     sentiment_stats(df)\n",
    "#     create_dfm(df)\n",
    "#     write_evaluations(args.test_file, args.output_path)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#*************************************************************************\n",
    "## Split up the corpus in positive and negative comments\n",
    "#\n",
    "# --> maybe rather one corpus with most positive and negative comments?!\n",
    "#\n",
    "#fb_s.POSITIVE <- subset(fb_s, Sentiment >= 0.20)\n",
    "#fb_s.NEGATIVE <- subset(fb_s, Sentiment <= -0.20)\n",
    "\n",
    "#fb_s.POSITIVE$message <- as.character(fb_s.POSITIVE$message)\n",
    "#fb_s.NEGATIVE$message <- as.character(fb_s.NEGATIVE$message)\n",
    "##\n",
    "#*********************************************************************************************************************\n",
    "#              High Subjectivity Comments\n",
    "#*********************************************************************************************************************\n",
    "# Get long comments with high sentiment (+/-) AND high subjectivity score\n",
    "\n",
    "fb_subjective <- fb_s[(fb_s$Sentiment >= 0.25) | (fb_s$Sentiment <= -0.25), ]\n",
    "\n",
    "fb_subjective <- subset(fb_subjective, Subjectivity >= 0.4)\n",
    "\n",
    "#Drop rows with very short comments\n",
    "fb_subjective$message <- as.character(fb_subjective$message)\n",
    "fb_subjective = fb_subjective[(which(nchar(fb_subjective$message) >= 250)),]\n",
    "\n",
    "#Drop less important columns\n",
    "fb_subjective = subset(fb_subjective, select = -c(X.U.FEFF.level, object_type,query_status,query_time, \n",
    "                                                  query_type) )\n",
    "\n",
    "#Sort by Subjectivity decreasing before start reading comments\n",
    "fb_subjective <-fb_subjective[order(fb_subjective$Subjectivity, decreasing = TRUE),]\n",
    "head(fb_subjective)\n",
    "\n",
    "#Subset for Teun for intercoder reliability \n",
    "fb_subset <-fb_subjective[121:250,]\n",
    "\n",
    "install.packages(\"writexl\")\n",
    "library(\"writexl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a text corpus\n",
    "quantedacorpus <- corpus(fb,\n",
    "                         text_field = \"message\",\n",
    "                         meta = list(\"id\", \"created_time\", \"comment_count\",\"Like_count\" ),\n",
    "                         unique_docnames = TRUE)\n",
    "#Prepare data\n",
    "#Stopwords\n",
    "stopwords('dutch')\n",
    "#more extensive list of dutch stopwords:\n",
    "mystopwords <- read.table(\"stop_words_dutch.txt\", header = TRUE)\n",
    "class(mystopwords)\n",
    "stop_vec = as.vector(mystopwords$Custom_stopwords)\n",
    "class(stop_vec)\n",
    "#Also drop basic English stopwords\n",
    "stopwords('english')\n",
    "#List of emojis & symbols\n",
    "emoji <- read.table(\"stop_words_emojis.txt\", header=TRUE)\n",
    "emoji_vec = as.vector(emoji$Emojis)\n",
    "\n",
    "\n",
    "# Create document-frequency-matrix for topic model\n",
    "dfm_fb <- dfm(quantedacorpus, \n",
    "                  tolower = TRUE, \n",
    "                  remove_numbers = TRUE, \n",
    "                  remove_punct = TRUE, \n",
    "                  remove_url = TRUE, \n",
    "                  remove_symbols = TRUE,\n",
    "                  remove = c(stop_vec,emoji_vec, stopwords('dutch'),stopwords('english')),\n",
    "                  stem = TRUE)\n",
    "\n",
    "######Explore\n",
    "#Get most frequent terms\n",
    "topfeatures(\n",
    "  dfm_fb,\n",
    "  n = 200,\n",
    "  decreasing = TRUE,\n",
    "  scheme = c(\"count\", \"docfreq\"),\n",
    "  groups = NULL\n",
    ")\n",
    "\n",
    "#Look at keywords in context\n",
    "head(kwic(quantedacorpus, pattern = \"go*\", window = 3, valuetype = \"glob\"))\n",
    "head(kwic(quantedacorpus, pattern = \"allergi*\", window = 6, valuetype = \"glob\"), n=15)\n",
    "allergy <- data.frame(head(kwic(quantedacorpus, pattern = \"allergi*\", window = 7, valuetype = \"glob\"), n=50))\n",
    "allergy\n",
    "\n",
    "head(kwic(quantedacorpus, pattern = \"bloedverdunner*\", window = 6, valuetype = \"glob\"), n=15)\n",
    "head(kwic(quantedacorpus, pattern = \"zwanger*\", window = 6, valuetype = \"glob\"), n=15)\n",
    "head(kwic(quantedacorpus, pattern = \"kinderwens\", window = 6, valuetype = \"glob\"), n=15)\n",
    "\n",
    "\n",
    "#Read sample patient reports\n",
    "options(max.print=1000)\n",
    "print(as.character(quantedacorpus[30]))\n",
    "print(as.character(quantedacorpus[152]))\n",
    "print(as.character(quantedacorpus[1540]))\n",
    "\n",
    "#most frequent bigrams\n",
    "#install.packages(\"tidytext\")\n",
    "#library(tidytext)\n",
    "#library(dplyr)\n",
    "#?unnest_tokens()\n",
    "#bifb <- fb %>% unnest_tokens(ngram,message,token = \"ngrams\", n = 2)\n",
    "#bifb %>% count(ngram, sort = TRUE)\n",
    "\n",
    "#***********************************************************************************************************\n",
    "## Further NLP: POS tagging and most frequent NOUN / ADJ\n",
    "# First clean quanteda corpus (similar to pre-processing steps as above for dfm)\n",
    "qcorpus <- tokens(quantedacorpus)\n",
    "\n",
    "qcorpus <- tokens_select(qcorpus, c(stop_vec,emoji_vec, stopwords('dutch'),stopwords('english')),selection='remove')\n",
    "\n",
    "qcorpus <- tokens(qcorpus, remove_numbers = TRUE,  remove_punct = TRUE,remove_url = TRUE, \n",
    "                  remove_symbols = TRUE,remove_separators = FALSE)\n",
    "      \n",
    "######## POS tagging and keyword analysis with udpipe #####################\n",
    "\n",
    "\n",
    "install.packages(\"udpipe\")\n",
    "library(udpipe)\n",
    "dl <- udpipe_download_model(language = \"dutch\")\n",
    "str(dl)\n",
    "\n",
    "udmodel_dutch <- udpipe_load_model(file = \"dutch-alpino-ud-2.5-191206.udpipe\")\n",
    "\n",
    "txt <- sapply(qcorpus, FUN=function(x){\n",
    "  x <- gsub(\" \", intToUtf8(160), x) ## replace space with no-break-space\n",
    "  paste(x, collapse = \" \")\n",
    "})\n",
    "x <- udpipe_annotate(udmodel_dutch, x = as.character(txt), tokenizer = \"horizontal\",parser = \"none\")\n",
    "x <- as.data.frame(x)\n",
    "str(x)\n",
    "table(x$upos)\n",
    "\n",
    "library(lattice)\n",
    "stats <- subset(x, upos %in% c(\"NOUN\")) \n",
    "stats <- txt_freq(stats$token)\n",
    "stats$key <- factor(stats$key, levels = rev(stats$key))\n",
    "barchart(key ~ freq, data = head(stats, 20), col = \"cadetblue\", \n",
    "         main = \"Most occurring nouns\", xlab = \"Freq\")\n",
    "\n",
    "stats <- subset(x, upos %in% c(\"VERB\")) \n",
    "stats <- txt_freq(stats$token)\n",
    "stats$key <- factor(stats$key, levels = rev(stats$key))\n",
    "barchart(key ~ freq, data = head(stats, 20), col = \"cadetblue\", \n",
    "         main = \"Most occurring verbs\", xlab = \"Freq\")\n",
    "\n",
    "stats <- subset(x, upos %in% c(\"ADJ\")) \n",
    "stats <- txt_freq(stats$token)\n",
    "stats$key <- factor(stats$key, levels = rev(stats$key))\n",
    "barchart(key ~ freq, data = head(stats, 20), col = \"cadetblue\", \n",
    "         main = \"Most occurring adjectives\", xlab = \"Freq\")\n",
    "\n",
    "\n",
    "### Extract top keyword NOUN - ADJ / NOUN - VERB combinations\n",
    "## Collocation (words following one another)\n",
    "stats <- keywords_collocation(x = x, \n",
    "                              term = \"token\", group = c(\"doc_id\", \"paragraph_id\", \"sentence_id\"),\n",
    "                              ngram_max = 4)\n",
    "collocs <- stats[order(-stats$freq),]\n",
    "collocs$keyword <- factor(collocs$keyword, levels = rev(collocs$keyword))\n",
    "barchart(keyword ~ freq, data = head(collocs, 20), col = \"cadetblue\", \n",
    "         main = \"Most occurring collocations\", xlab = \"Freq\")\n",
    "\n",
    "## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives\n",
    "stats <- cooccurrence(x = subset(x, upos %in% c(\"NOUN\", \"ADJ\")), \n",
    "                      term = \"lemma\", group = c(\"doc_id\", \"paragraph_id\", \"sentence_id\"))\n",
    "\n",
    "co_occur <- stats[order(-stats$cooc),]\n",
    "\n",
    "\n",
    "## Co-occurrences: How frequent do words follow one another\n",
    "stats <- cooccurrence(x = x$lemma, \n",
    "                      relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n",
    "\n",
    "co_occur <- data.frame(stats[order(-stats$cooc),])\n",
    "library(tidyr)\n",
    "co_occur <- co_occur %>% unite(\"words\", term1:term2, sep = \" \", remove = FALSE)\n",
    "co_occur$words <- factor(co_occur$words, levels = rev(co_occur$words))\n",
    "barchart(words ~ cooc, data = head(co_occur, 20), col = \"cadetblue\", \n",
    "         main = \"Most occurring co-occurences\", xlab = \"Freq\")\n",
    "\n",
    "\n",
    "## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between\n",
    "stats <- cooccurrence(x = x$lemma, \n",
    "                      relevant = x$upos %in% c(\"NOUN\", \"ADJ\"), skipgram = 2)\n",
    "head(stats)\n",
    "\n",
    "## Visualizations\n",
    "library(igraph)\n",
    "library(ggraph)\n",
    "library(ggplot2)\n",
    "wordnetwork <- head(stats, 70)\n",
    "wordnetwork <- graph_from_data_frame(wordnetwork)\n",
    "ggraph(wordnetwork, layout = \"fr\") +\n",
    "  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = \"pink\") +\n",
    "  geom_node_text(aes(label = name), col = \"darkgreen\", size = 4) +\n",
    "  theme_graph(base_family = \"Arial Narrow\") +\n",
    "  theme(legend.position = \"none\") +\n",
    "  labs(title = \"Cooccurrences within 3 words distance\", subtitle = \"Nouns & Adjective\")\n",
    "\n",
    "## Using RAKE\n",
    "stats <- keywords_rake(x = x, term = \"lemma\", group = \"doc_id\", \n",
    "                       relevant = x$upos %in% c(\"NOUN\", \"ADJ\"))\n",
    "stats$key <- factor(stats$keyword, levels = rev(stats$keyword))\n",
    "barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = \"red\", \n",
    "         main = \"Keywords identified by RAKE\", \n",
    "         xlab = \"Rake\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
