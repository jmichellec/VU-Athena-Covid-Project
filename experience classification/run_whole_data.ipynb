{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ae934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from OpenDutchWordnet import Wn_grid_parser\n",
    "import argparse\n",
    "import re\n",
    "import nltk\n",
    "import xgboost\n",
    "import re\n",
    "import numpy as np\n",
    "# import stanza\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a5f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        print(text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    no_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove urls\n",
    "    no_urls = re.sub(r\"http\\S+\", \"\", no_emoji)\n",
    "    \n",
    "    # Remove punctuation, numbers and symbols\n",
    "    no_punct_symbols_nrs = re.sub(r'[^A-Za-z\\s]+', '', no_urls)\n",
    "    \n",
    "    # Remove trailing white space\n",
    "    no_trailing_ws = \" \".join(no_punct_symbols_nrs.split())\n",
    "    \n",
    "    # Lowercase\n",
    "    text_clean = no_trailing_ws.lower()\n",
    "    return text_clean\n",
    "\n",
    "def lemmatize(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f59561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(nlp, df_train, df_test):\n",
    "    # Clean text\n",
    "    df_train['clean_text'] = df_train['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "    # Remove empty values\n",
    "    df_train = df_train[df_train['clean_text'] != '']\n",
    "\n",
    "    # Lemmatize \n",
    "    df_train['lemmatized_clean_text'] = df_train['clean_text'].apply(lambda x: lemmatize(nlp, x))    \n",
    "\n",
    "    df_test['clean_text'] = df_test['message'].apply(lambda x: clean_text(x))\n",
    "    df_test = df_test[df_test['clean_text'] != '']        \n",
    "    df_test['lemmatized_clean_text'] = df_test['clean_text'].apply(lambda x: lemmatize(nlp, x))  \n",
    "    \n",
    "    # Binary labels\n",
    "    df_test['labels'].replace({\"y\": 1, \"n\": 0}, inplace=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb987416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms (instance, synset_id, hypers):\n",
    "    synset = instance.synsets_find_synset(synset_id)\n",
    "    if synset:\n",
    "        hypernyms = synset.get_relations(\"has_hyperonym\")\n",
    "        if hypernyms:\n",
    "            for h in hypernyms:\n",
    "                if (h):\n",
    "                    if not h.get_target() in hypers:\n",
    "                        hypers.append(h.get_target())\n",
    "                        get_hypernyms(instance, h.get_target(), hypers)\n",
    "\n",
    "\n",
    "def get_hypernyms_lemmas():\n",
    "    # ['ontmoeten',\n",
    "    #  'voelen',\n",
    "    #  'meemaken',\n",
    "    #  'ondervinden',\n",
    "    #  'ondergaan',\n",
    "    #  'gevoelen',\n",
    "    #  'zich omkleden',\n",
    "    #  'ervaren',\n",
    "    #  'gewaarworden',\n",
    "    #  'kenteren',\n",
    "    #  'doorleven',\n",
    "    #  'veranderen',\n",
    "    #  'keren',\n",
    "    #  'beleven']\n",
    "    instance = Wn_grid_parser(path_wn_grid_lmf='odwn_orbn_gwg-LMF_1.3.xml.gz')\n",
    "    le_el = instance.les_find_le(\"voelen-v-2\")\n",
    "    synset_el = instance.synsets_find_synset(le_el.get_synset_id())\n",
    "    hypers = []\n",
    "    get_hypernyms(instance, synset_el.get_id(), hypers)\n",
    "    new_hypers = []\n",
    "\n",
    "    lemmas = []\n",
    "    for hyper in hypers:\n",
    "        new_hypers.append(hyper)\n",
    "        for le in instance.les_all_les_of_one_synset(hyper):\n",
    "            lemmas.append(le.get_lemma())  \n",
    "\n",
    "    for hyper in new_hypers:\n",
    "        hypers = []\n",
    "        get_hypernyms(instance, hyper, hypers)\n",
    "        new_hypers = []\n",
    "        for hyper in hypers:\n",
    "            new_hypers.append(hyper)\n",
    "            for le in instance.les_all_les_of_one_synset(hyper):\n",
    "                lemmas.append(le.get_lemma())\n",
    "                \n",
    "    return list(set(lemmas+['voelen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c29084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match(match, clean_text):\n",
    "    if re.search(match, clean_text):\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def lemma_replace(word_list, clean_text, lemmatized_clean_text):\n",
    "    clean_text_tokenized = clean_text.split()\n",
    "    for lemma_word in word_list:\n",
    "        if lemma_word in lemmatized_clean_text:\n",
    "            for i, word in enumerate(lemmatized_clean_text.split()):\n",
    "                if word == lemma_word:\n",
    "                    clean_text_tokenized[i] = clean_text_tokenized[i].replace(clean_text_tokenized[i], lemma_word)\n",
    "    clean_text = \" \".join(clean_text_tokenized)\n",
    "    return clean_text\n",
    "\n",
    "def heuristics_labelling(df_train, df_test, WN_synsets, filters, remove):\n",
    "    \n",
    "    df_train['clean_text_lemma'] = df_train['clean_text']\n",
    "    df_test['clean_text_lemma'] = df_test['clean_text']\n",
    "    \n",
    "    matches = []\n",
    "    if 0 in filters:\n",
    "        matches.append(\"((heb|heeft|hebben) [a-z]* (gehad))\")\n",
    "    if 1 in filters:\n",
    "        direct_relation = 'vader|moeder|ouder|schoonvader|schoonmoeder|kind|zoon|dochter|man|vrouw|broer|zus|neef|nicht|tante|oom'\n",
    "        df_train['clean_text_lemma'] = df_train[['clean_text', 'lemmatized_clean_text']].apply(lambda x: lemma_replace(direct_relation.split('|'), x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "        df_test['clean_text_lemma'] = df_test[['clean_text', 'lemmatized_clean_text']].apply(lambda x: lemma_replace(direct_relation.split('|'), x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "        \n",
    "        matches.append(\"((mijn|mn|me|m n|mij|men) \" + '(' + direct_relation + '))') \n",
    "                       \n",
    "    if 2 in filters:                       \n",
    "        df_train['clean_text_lemma'] = df_train[['clean_text', 'lemmatized_clean_text']].apply(lambda x: lemma_replace(WN_synsets, x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "        df_test['clean_text_lemma'] = df_test[['clean_text', 'lemmatized_clean_text']].apply(lambda x: lemma_replace(WN_synsets, x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "                \n",
    "        matches.append('('+'|'.join(WN_synsets)+')')\n",
    "\n",
    "    match = '|'.join(matches)\n",
    "    df_train['labels'] = df_train['clean_text_lemma'].apply(lambda x: check_match(match, x))\n",
    "    df_test['predicted'] = df_test['clean_text_lemma'].apply(lambda x: check_match(match, x))\n",
    "      \n",
    "    if remove == True:\n",
    "        df_train['clean_text_removals'] = df_train['clean_text_lemma'].apply(lambda x: re.sub(match, '', x))\n",
    "        df_test['clean_text_removals'] = df_test['clean_text_lemma'].apply(lambda x: re.sub(match, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd049f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df_train, df_train_labels):\n",
    "    over = RandomOverSampler(sampling_strategy=1, random_state=42)\n",
    "    df_train_sampled, df_train_sampled_labels = over.fit_resample(df_train, df_train_labels)\n",
    "    return df_train_sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26555487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_train, df_test, vectorizer, remove):\n",
    "    vect = vectorizer\n",
    "    y_train = df_train['labels']\n",
    "    y_test = df_test['labels']\n",
    "    if remove == True:\n",
    "        corpus = df_train['clean_text_removals'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text_removals'])\n",
    "    else:\n",
    "        corpus = df_train['clean_text'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text'])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60dc656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_unlabelled(df_train, df_test, vectorizer, remove):\n",
    "    vect = vectorizer\n",
    "    y_train = df_train['labels']\n",
    "    if remove == True:\n",
    "        corpus = df_train['clean_text_removals'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text_removals'])\n",
    "    else:\n",
    "        corpus = df_train['clean_text'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text'])\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfb83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cm(true_labels, predicted_labels):\n",
    "    cm = metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "        \n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                    colorbar=False,\n",
    "                                    show_absolute=False,\n",
    "                                    show_normed=True,\n",
    "                                    class_names=['non-exp','exp'])\n",
    "    fig.set_size_inches(10, 10.5)\n",
    "\n",
    "    \n",
    "def class_feature_importance(X, Y, feature_importances, vect):\n",
    "    N, M = X.shape\n",
    "    X = scale(X, with_mean=False)\n",
    "\n",
    "    out = {}\n",
    "    for c in set(Y):\n",
    "        out[c] = dict(\n",
    "            zip(vect.get_feature_names(), np.mean(X[Y==c, :], axis=0)*feature_importances)\n",
    "        )\n",
    "\n",
    "    return out    \n",
    "\n",
    "def classification_experiments(X_train, y_train, X_test, y_test):\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Logistic Regression\")\n",
    "    \n",
    "    clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"LR: ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "#     create_cm(y_test, y_pred)\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"XGBoost Random Forest\")\n",
    "    \n",
    "    xgbc = XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric='logloss')\n",
    "    xgb = xgbc.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"XGB: ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "#     create_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4759496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_train_test_split(full_df, sub_df, test_df):\n",
    "    test_ids = test_df['object_id'].tolist()\n",
    "    sub_ids = sub_df['object_id'].tolist()\n",
    "    print(len(full_df))\n",
    "    full_df = full_df[~full_df.object_id.isin(test_ids)]\n",
    "    print(len(full_df))\n",
    "    full_df = full_df[~full_df.object_id.isin(sub_ids)]\n",
    "    print(len(full_df))\n",
    "    sub_df = sub_df[~sub_df.object_id.isin(test_ids)]\n",
    "    print(len(sub_df))\n",
    "    # full_df_test is for prediction\n",
    "    full_df_train_no_sentiment, full_df_test = train_test_split(full_df, test_size=0.42, random_state=42)\n",
    "    print(len(full_df_train_no_sentiment))\n",
    "    full_df_train = pd.concat([full_df_train_no_sentiment, sub_df]) #sub_df  is around 2 percent\n",
    "    return full_df_train, full_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e807dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicted(unlabelled_df, unlabelled_X_test, best_model, name):\n",
    "    predictions = best_model.predict_proba(unlabelled_X_test)\n",
    "    unlabelled_df['best_model_pred'] = predictions\n",
    "    unlabelled_df[['text', 'best_model_pred']].to_csv(name+'.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165e2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('lemmatized_train_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "# sub_df = pd.read_csv('lemmatized_train_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "# df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "# df_train, df_unlabelled = full_data_train_test_split(df_train, sub_df, df_test)\n",
    "# print(len(df_train))\n",
    "# print(len(df_unlabelled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68553b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemmatized_files():\n",
    "    df_train = pd.read_csv('fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.csv', sep='\\t')\n",
    "    df_test = pd.read_csv('experience_test/Fb_random_sample_500_annotated_discussed.tsv', sep='\\t')\n",
    "    sub_df = pd.read_csv('high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.csv', sep='\\t')\n",
    "    \n",
    "    df_train = df_train[~df_train['text'].isna()]\n",
    "    df_test = df_test[~df_test['message'].isna()]\n",
    "    sub_df = sub_df[~sub_df['text'].isna()]\n",
    "    \n",
    "    nlp = stanza.Pipeline(lang='nl', processors='tokenize,pos,lemma')\n",
    "    df_train_sent, df_test_sent = process_data(nlp, sub_df, df_test)\n",
    "    print(\"Finished sentiment\")\n",
    "    df_train, df_test = process_data(nlp, df_train, df_test)\n",
    "    print(\"Finished train and test\")\n",
    " \n",
    "    df_test_sent.to_csv('lemmatized_test_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_train_sent.to_csv('lemmatized_train_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "    df_train.to_csv('lemmatized_train_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t') # no test \n",
    "    df_test.to_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23fff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_models():\n",
    "    # comment this when not using unlabelled    \n",
    "    df_train = pd.read_csv('lemmatized_train_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    sub_df = pd.read_csv('lemmatized_train_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    \n",
    "    df_train, df_unlabelled = full_data_train_test_split(df_train, sub_df, df_test)\n",
    "    df_train.to_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t') \n",
    "    df_unlabelled.to_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "\n",
    "    df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "#     df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "    WN_synsets = get_hypernyms_lemmas()\n",
    "    \n",
    "    vectorizers = [TfidfVectorizer(), CountVectorizer(binary=True)]\n",
    "    removes = [False, True]\n",
    "    filters = [[0], [1], [2], [0, 1], [1, 2], [0, 2], [0, 1, 2]]\n",
    "    for vect in vectorizers:\n",
    "        print(vect)\n",
    "        print('-----------------')\n",
    "        for remove in removes:\n",
    "            print(remove)\n",
    "            print('-----------------')\n",
    "            for filter_ in filters:\n",
    "                print(filter_)\n",
    "                print('-------------------')\n",
    "                heuristics_labelling(df_train, df_test, WN_synsets, filter_, remove)\n",
    "                print('DATA STATS: ', df_train['labels'].value_counts())\n",
    "\n",
    "                df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "                X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, vect, remove)\n",
    "                print('-------------------')\n",
    "                print(\"BASELINE\")\n",
    "                print(classification_report(df_test['labels'], df_test['predicted']))\n",
    "                classification_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70719b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_labelling(df_train, df_test, df_unlabelled_test, WN_synsets, best_xgb):\n",
    "    # Best XGB \n",
    "    heuristics_labelling(df_train, df_unlabelled_test, WN_synsets, [0,1,2], False)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    bin_vect = CountVectorizer(binary=True)\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, bin_vect, False)\n",
    "\n",
    "    _, _, unlabelled_X_test = create_train_test_unlabelled(df_train_sampled, df_unlabelled_test, bin_vect, False)\n",
    "    \n",
    "    predictions = best_xgb.predict_proba(unlabelled_X_test)[:, 1]\n",
    "    predictions_labels = best_xgb.predict(unlabelled_X_test)\n",
    "\n",
    "    df_unlabelled_test['best_model_pred'] = predictions\n",
    "    df_unlabelled_test['best_model_pred_labels'] = predictions_labels\n",
    "#     df_unlabelled_test[['text', 'best_model_pred']].to_csv('predicted_data/unlabelled_predictions_by_xgb-binary-keepfeat-1.tsv', sep='\\t', index=False)\n",
    "    df_unlabelled_test[['text', 'best_model_pred', 'best_model_pred_labels']].to_csv('corrections/unlabelled_prediction_labels_by_xgb-binary-keepfeat-1.tsv', sep='\\t', index=False, header=None)\n",
    "\n",
    "def best_xgb_results(df_train, df_test, WN_synsets):\n",
    "    heuristics_labelling(df_train, df_test, WN_synsets, [0,1,2], False)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    bin_vect = CountVectorizer(binary=True)\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, bin_vect, False)\n",
    "    best_xgb = XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric='logloss').fit(X_train, y_train)  \n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    importance = best_xgb.feature_importances_\n",
    "\n",
    "    result = class_feature_importance(X_test.toarray(), y_pred, importance, bin_vect)\n",
    "\n",
    "    d = result.get(1)\n",
    "\n",
    "    sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_d[:20])\n",
    "    \n",
    "    return best_xgb, y_pred, y_test\n",
    "    \n",
    "def best_lr_results(df_train, df_test, WN_synsets):\n",
    "    heuristics_labelling(df_train, df_test, WN_synsets, [0,1,2], True)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, tfidf, False)\n",
    "    clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Logistic Regression\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    importance = clf.coef_[0]\n",
    "\n",
    "    result = class_feature_importance(X_test.toarray(), y_pred, importance, tfidf)\n",
    "\n",
    "    d = result.get(1)\n",
    "\n",
    "    sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(sorted_d[:20])\n",
    "    \n",
    "    return clf, y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11098803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dfs():\n",
    "    bm_df = pd.read_csv('predicted_data/unlabelled_predictions_by_xgb-binary-keepfeat-1.tsv', sep='\\t', header=None)\n",
    "#     bm_df[:250].to_csv('predicted_data/top-250-unlabelled_predictions_by_xgb-binary-keepfeat.tsv', sep='\\t', header=False)\n",
    "\n",
    "#     bm_df_unlabelled = bm_df.fillna('Unlabelled')\n",
    "#     labelled = bm_df_unlabelled[bm_df_unlabelled[3] != 'Unlabelled']\n",
    "#     labelled[[0, 1, 3]].to_csv('predicted_data/workshop_labelled_predictions_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=False)\n",
    "\n",
    "#     bm_df_unlabelled = bm_df[~bm_df[1].isin(labelled.index)]\n",
    "#     above_unlabelled_09 = bm_df_unlabelled[bm_df_unlabelled[1] >= 0.9]\n",
    "#     above_unlabelled_09_sample = above_unlabelled_09.sample(n=250, random_state=50)\n",
    "#     above_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_over0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=False, index=False)\n",
    "    \n",
    "#     below_unlabelled_09 = bm_df_unlabelled[bm_df_unlabelled[1] < 0.9]\n",
    "#     below_unlabelled_09 = below_unlabelled_09[below_unlabelled_09[1] >= 0.5]\n",
    "#     below_unlabelled_09_sample = below_unlabelled_09.sample(n=250, random_state=50)\n",
    "#     below_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_below0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=False, index=False)\n",
    "\n",
    "    above_unlabelled_09 = bm_df[bm_df[1] >= 0.9]\n",
    "    above_unlabelled_09_sample = above_unlabelled_09.sample(n=250, random_state=50)\n",
    "    above_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_over0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=False, index=False)\n",
    "    \n",
    "    below_unlabelled_09 = bm_df[bm_df[1] < 0.9]\n",
    "    below_unlabelled_09 = below_unlabelled_09[below_unlabelled_09[1] >= 0.5]\n",
    "    below_unlabelled_09_sample = below_unlabelled_09.sample(n=250, random_state=50)\n",
    "    below_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_below0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485bd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_preds_index(y_test, y_pred):\n",
    "    return [1 if i == j else 0 for i, j in zip(y_test, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f6125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dfed9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6b2c59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68953"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr.coef_.shape[-1]\n",
    "len(xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c80434a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       451\n",
      "           1       0.38      0.49      0.43        49\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.66      0.70      0.68       500\n",
      "weighted avg       0.89      0.87      0.88       500\n",
      "\n",
      "[[412  39]\n",
      " [ 25  24]]\n",
      "[('mijn', 37.5394589326804), ('veranderen', 15.195921665435964), ('voelen', 14.143905054037504), ('moeder', 10.696318957685547), ('gehad', 8.160789016091105), ('en', 7.0674213730451445), ('ouders', 6.775077811674564), ('vader', 6.226580536516842), ('kind', 5.317612302921842), ('de', 4.173522355097137), ('kinderen', 4.117563336583467), ('dochter', 3.9191108606087375), ('mn', 3.5923236220597317), ('heb', 3.2703007162048428), ('ontmaskeren', 3.1096909578192182), ('ook', 3.062119901102135), ('ik', 2.8111040196731327), ('van', 2.7796139972246197), ('ondervinden', 2.6695382279199324), ('te', 2.6206176149749996)]\n",
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       451\n",
      "           1       0.36      0.55      0.44        49\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.66      0.72      0.68       500\n",
      "weighted avg       0.89      0.86      0.87       500\n",
      "\n",
      "[[404  47]\n",
      " [ 22  27]]\n",
      "[('mijn', 0.07606954745757265), ('en', 0.040133188615727136), ('gehad', 0.015218279679826183), ('het', 0.015060751687811774), ('die', 0.01478758952494729), ('voelen', 0.013125200847761283), ('veranderen', 0.011746545175977015), ('ook', 0.010378894536077793), ('de', 0.01019848904808294), ('te', 0.007089895940134645), ('in', 0.0069052894501721186), ('moeder', 0.006366589741507317), ('is', 0.005035260734763819), ('we', 0.004713656978421391), ('heb', 0.004007162248918236), ('ik', 0.0033331273668041977), ('ze', 0.003281975484730183), ('best', 0.0030982258200012262), ('zijn', 0.0030839834713933677), ('man', 0.0028300290842856422)]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "df_unlabelled_test = pd.read_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "WN_synsets = get_hypernyms_lemmas()\n",
    "\n",
    "#     create_lemmatized_files()\n",
    "#     results_models()\n",
    "print(\"LR\")\n",
    "lr, y_pred_lr, y_test = best_lr_results(df_train, df_test, WN_synsets)\n",
    "# lr_correct = correct_preds_index(y_test, y_pred_lr)\n",
    "\n",
    "print(\"XGB\")\n",
    "xgb, y_pred_xgb, y_test = best_xgb_results(df_train, df_test, WN_synsets)\n",
    "# xgb_correct = correct_preds_index(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc6c424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "    df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_unlabelled_test = pd.read_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "    WN_synsets = get_hypernyms_lemmas()\n",
    "    \n",
    "#     create_lemmatized_files()\n",
    "    results_models()\n",
    "    print(\"LR\")\n",
    "    lr, y_pred_lr, y_test = best_lr_results(df_train, df_test, WN_synsets)\n",
    "    lr_correct = correct_preds_index(y_test, y_pred_lr)\n",
    "    \n",
    "    print(\"XGB\")\n",
    "    xgb, y_pred_xgb, y_test = best_xgb_results(df_train, df_test, WN_synsets)\n",
    "    xgb_correct = correct_preds_index(y_test, y_pred_xgb)\n",
    "\n",
    "\n",
    "    print(\"correct matrix\")\n",
    "    print(confusion_matrix(lr_correct, xgb_correct))\n",
    "    \n",
    "    print(\"prediction matrix\")\n",
    "    print(confusion_matrix(y_pred_lr, y_pred_xgb))\n",
    "    \n",
    "    prediction_labelling(df_train, df_test, df_unlabelled_test, WN_synsets, xgb)\n",
    "    create_sample_dfs()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b155190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_below = pd.read_csv('predicted_data/sample_unlabelledpredictions_below0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=None)\n",
    "new_above = pd.read_csv('predicted_data/sample_unlabelledpredictions_over0-9_by_xgb-binary-keepfeat-1.csv', sep='\\t', header=None)\n",
    "old_below = pd.read_csv('predicted_data/sample_unlabelledpredictions_below0-9_by_xgb-binary-keepfeat.tsv', sep='\\t', header=None)\n",
    "old_above = pd.read_csv('predicted_data/sample_unlabelledpredictions_over0-9_by_xgb-binary-keepfeat.tsv', sep='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46148d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n    449\n",
       "y     51\n",
       "Name: Experience knowledge & value considerations yes / no (S), dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('experience_test/Fb_random_sample_500_annotated_discussed.tsv', sep='\\t')\n",
    "df_test['Experience knowledge & value considerations yes / no (S)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db91d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3417: DtypeWarning: Columns (24,25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       451\n",
      "           1       0.36      0.55      0.44        49\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.66      0.72      0.68       500\n",
      "weighted avg       0.89      0.86      0.87       500\n",
      "\n",
      "[[404  47]\n",
      " [ 22  27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mijn', 0.07606954745757265), ('en', 0.040133188615727136), ('gehad', 0.015218279679826183), ('het', 0.015060751687811774), ('die', 0.01478758952494729), ('voelen', 0.013125200847761283), ('veranderen', 0.011746545175977015), ('ook', 0.010378894536077793), ('de', 0.01019848904808294), ('te', 0.007089895940134645), ('in', 0.0069052894501721186), ('moeder', 0.006366589741507317), ('is', 0.005035260734763819), ('we', 0.004713656978421391), ('heb', 0.004007162248918236), ('ik', 0.0033331273668041977), ('ze', 0.003281975484730183), ('best', 0.0030982258200012262), ('zijn', 0.0030839834713933677), ('man', 0.0028300290842856422)]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "df_unlabelled_test = pd.read_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb-1.tsv', sep='\\t')\n",
    "\n",
    "WN_synsets = get_hypernyms_lemmas()\n",
    "\n",
    "xgb, y_pred_xgb, y_test = best_xgb_results(df_train, df_test, WN_synsets)\n",
    "\n",
    "prediction_labelling(df_train, df_test, df_unlabelled_test, WN_synsets, xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd7f9f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    435\n",
       "0     65\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_df = pd.read_csv('corrections/unlabelled_prediction_labels_by_xgb-binary-keepfeat-1.tsv', sep='\\t', header=None)\n",
    "below = pd.merge(bm_df, old_below, how='inner', on=[0])\n",
    "above = pd.merge(bm_df, old_above, how='inner', on=[0])\n",
    "# bm_df[2].value_counts()\n",
    "pd.concat([above, below])[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24d1b2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. AD 2016 griep, (zie Google) zelfde symptome...</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC Ferhat Ik ben ettelijke malen ingeent, en i...</td>\n",
       "      <td>0.999959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soraya van Berkel  Deze oproep geldt voor elke...</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deze oproep geldt voor elke land, niet alleen ...</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angelique Raadgever sterk tegenargument. Ik we...</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48811</th>\n",
       "      <td>Pieter Koopman  laten we maar hopen dat onze o...</td>\n",
       "      <td>0.010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48812</th>\n",
       "      <td>Bedankt zelfs deze mensen die weten meer dan r...</td>\n",
       "      <td>0.010387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48813</th>\n",
       "      <td>Kunnen die ouders niet aanklaagt worden? Dat v...</td>\n",
       "      <td>0.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48814</th>\n",
       "      <td>Shie Do jij bent zo maf als een mandje zeker g...</td>\n",
       "      <td>0.009261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48815</th>\n",
       "      <td>Jiv Bansi jammer dat uw ouders u gevaccineerd ...</td>\n",
       "      <td>0.009261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48816 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0         1\n",
       "0      1. AD 2016 griep, (zie Google) zelfde symptome...  0.999988\n",
       "1      TC Ferhat Ik ben ettelijke malen ingeent, en i...  0.999959\n",
       "2      Soraya van Berkel  Deze oproep geldt voor elke...  0.999935\n",
       "3      Deze oproep geldt voor elke land, niet alleen ...  0.999935\n",
       "4      Angelique Raadgever sterk tegenargument. Ik we...  0.999906\n",
       "...                                                  ...       ...\n",
       "48811  Pieter Koopman  laten we maar hopen dat onze o...  0.010394\n",
       "48812  Bedankt zelfs deze mensen die weten meer dan r...  0.010387\n",
       "48813  Kunnen die ouders niet aanklaagt worden? Dat v...  0.009838\n",
       "48814  Shie Do jij bent zo maf als een mandje zeker g...  0.009261\n",
       "48815  Jiv Bansi jammer dat uw ouders u gevaccineerd ...  0.009261\n",
       "\n",
       "[48816 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_df = pd.read_csv('predicted_data/unlabelled_predictions_by_xgb-binary-keepfeat-1.tsv', sep='\\t', header=None)\n",
    "below = pd.merge(bm_df, old_below, how='inner', on=[0])\n",
    "above = pd.merge(bm_df, old_above, how='inner', on=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843eb8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. AD 2016 griep, (zie Google) zelfde symptome...</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deze oproep geldt voor elke land, niet alleen ...</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angelique Raadgever sterk tegenargument. Ik we...</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sandra Haas toen was het minder duidelijk dat ...</td>\n",
       "      <td>0.999838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paula de Gunst mijn vader werd gebeld door de ...</td>\n",
       "      <td>0.999667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Kevin Dermaux Hou toch op dwaas. Ze moeten jul...</td>\n",
       "      <td>0.681436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Giny Jonkeren hoe vaak je het ook roept, zodat...</td>\n",
       "      <td>0.654695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Kees Beekhuijs Liever dat dan die rommel in mi...</td>\n",
       "      <td>0.625234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Dit is verhaal van mij vriendin Elisa Limas Ir...</td>\n",
       "      <td>0.555987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Nou laat ze dan ook nu maar es op visite bij h...</td>\n",
       "      <td>0.215318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0         1\n",
       "0    1. AD 2016 griep, (zie Google) zelfde symptome...  0.999988\n",
       "1    Deze oproep geldt voor elke land, niet alleen ...  0.999935\n",
       "2    Angelique Raadgever sterk tegenargument. Ik we...  0.999906\n",
       "3    Sandra Haas toen was het minder duidelijk dat ...  0.999838\n",
       "4    Paula de Gunst mijn vader werd gebeld door de ...  0.999667\n",
       "..                                                 ...       ...\n",
       "245  Kevin Dermaux Hou toch op dwaas. Ze moeten jul...  0.681436\n",
       "246  Giny Jonkeren hoe vaak je het ook roept, zodat...  0.654695\n",
       "247  Kees Beekhuijs Liever dat dan die rommel in mi...  0.625234\n",
       "248  Dit is verhaal van mij vriendin Elisa Limas Ir...  0.555987\n",
       "249  Nou laat ze dan ook nu maar es op visite bij h...  0.215318\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above = pd.merge(bm_df, old_above, how='inner', on=[0])\n",
    "above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ceaf835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "below = pd.merge(bm_df, old_below, how='inner', on=[0])\n",
    "above = pd.merge(bm_df, old_above, how='inner', on=[0])\n",
    "\n",
    "data = pd.read_csv('corrections/corrected_500_unlabelled.tsv', sep='\\t', header=None)[250:]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(data[1],bins=10,kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d3cd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "above = pd.merge(bm_df, old_above, how='inner', on=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64ae7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_df = pd.read_csv('predicted_data/unlabelled_predictions_by_xgb-binary-keepfeat-1.tsv', sep='\\t', header=None)\n",
    "above_unlabelled_09 = bm_df[bm_df[1] >= 0.9]\n",
    "\n",
    "below_unlabelled_09 = bm_df[bm_df[1] < 0.9]\n",
    "below_unlabelled_09 = below_unlabelled_09[below_unlabelled_09[1] >= 0.5]\n",
    "\n",
    "\n",
    "pd.merge(below_unlabelled_09, old_below, how='inner', on=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72366aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. AD 2016 griep, (zie Google) zelfde symptome...</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deze oproep geldt voor elke land, niet alleen ...</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angelique Raadgever sterk tegenargument. Ik we...</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sandra Haas toen was het minder duidelijk dat ...</td>\n",
       "      <td>0.999838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paula de Gunst mijn vader werd gebeld door de ...</td>\n",
       "      <td>0.999667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>mijn idee is toch om eerst de kwetsbaren te va...</td>\n",
       "      <td>0.515912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Martin Derby in mijn ogen nauwelijks. Ik mis v...</td>\n",
       "      <td>0.513940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Ik ook niet.. en ken ook weinig in mijn omgevi...</td>\n",
       "      <td>0.513735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Ze moeten deze mensen ook geen vaccin geven , ...</td>\n",
       "      <td>0.510170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Heb al zoveel vaccins in mijn lichaam zitten, ...</td>\n",
       "      <td>0.505297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0         1\n",
       "0    1. AD 2016 griep, (zie Google) zelfde symptome...  0.999988\n",
       "1    Deze oproep geldt voor elke land, niet alleen ...  0.999935\n",
       "2    Angelique Raadgever sterk tegenargument. Ik we...  0.999906\n",
       "3    Sandra Haas toen was het minder duidelijk dat ...  0.999838\n",
       "4    Paula de Gunst mijn vader werd gebeld door de ...  0.999667\n",
       "..                                                 ...       ...\n",
       "162  mijn idee is toch om eerst de kwetsbaren te va...  0.515912\n",
       "163  Martin Derby in mijn ogen nauwelijks. Ik mis v...  0.513940\n",
       "164  Ik ook niet.. en ken ook weinig in mijn omgevi...  0.513735\n",
       "165  Ze moeten deze mensen ook geen vaccin geven , ...  0.510170\n",
       "166  Heb al zoveel vaccins in mijn lichaam zitten, ...  0.505297\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhi = pd.merge(bm_df, old_above, how='inner', on=[0])\n",
    "hhi[hhi[1] >= 0.9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41c9a1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Die zijn gestoord je ongeboren kind als proefk...</td>\n",
       "      <td>0.899963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pepijn Noordenbos ja en daarna wordt je weer b...</td>\n",
       "      <td>0.893348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Waarom moeten gezonde mensen een vaccine nemen...</td>\n",
       "      <td>0.891940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mario Kapitein Echt...denk jij nu SERIEUS  dat...</td>\n",
       "      <td>0.889166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yolanda Truyen ik heb vorig jaar maart corona ...</td>\n",
       "      <td>0.887496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>mijn idee is toch om eerst de kwetsbaren te va...</td>\n",
       "      <td>0.515912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Martin Derby in mijn ogen nauwelijks. Ik mis v...</td>\n",
       "      <td>0.513940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Ik ook niet.. en ken ook weinig in mijn omgevi...</td>\n",
       "      <td>0.513735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Ze moeten deze mensen ook geen vaccin geven , ...</td>\n",
       "      <td>0.510170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Heb al zoveel vaccins in mijn lichaam zitten, ...</td>\n",
       "      <td>0.505297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0         1\n",
       "19   Die zijn gestoord je ongeboren kind als proefk...  0.899963\n",
       "20   Pepijn Noordenbos ja en daarna wordt je weer b...  0.893348\n",
       "21   Waarom moeten gezonde mensen een vaccine nemen...  0.891940\n",
       "22   Mario Kapitein Echt...denk jij nu SERIEUS  dat...  0.889166\n",
       "23   Yolanda Truyen ik heb vorig jaar maart corona ...  0.887496\n",
       "..                                                 ...       ...\n",
       "181  mijn idee is toch om eerst de kwetsbaren te va...  0.515912\n",
       "182  Martin Derby in mijn ogen nauwelijks. Ik mis v...  0.513940\n",
       "183  Ik ook niet.. en ken ook weinig in mijn omgevi...  0.513735\n",
       "184  Ze moeten deze mensen ook geen vaccin geven , ...  0.510170\n",
       "185  Heb al zoveel vaccins in mijn lichaam zitten, ...  0.505297\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhi = pd.merge(bm_df, old_below, how='inner', on=[0])\n",
    "hhi = hhi[hhi[1] < 0.9]\n",
    "hhi[hhi[1] >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00135824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. AD 2016 griep, (zie Google) zelfde symptome...</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deze oproep geldt voor elke land, niet alleen ...</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angelique Raadgever sterk tegenargument. Ik we...</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sandra Haas toen was het minder duidelijk dat ...</td>\n",
       "      <td>0.999838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paula de Gunst mijn vader werd gebeld door de ...</td>\n",
       "      <td>0.999667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Vind ik ook. Heb me wel weer laten inenten. Ma...</td>\n",
       "      <td>0.906661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Joops Eersel QEnergy alleen de antistoffen bli...</td>\n",
       "      <td>0.905045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Wanneer flikkeren  jullie op met deze dagelijk...</td>\n",
       "      <td>0.904138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Mariska Berkelaar-Smit heb in een zwaar gerefo...</td>\n",
       "      <td>0.903586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Ikke wel. Wat een gezever van iedereen. Ben bl...</td>\n",
       "      <td>0.903291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0         1\n",
       "0    1. AD 2016 griep, (zie Google) zelfde symptome...  0.999988\n",
       "1    Deze oproep geldt voor elke land, niet alleen ...  0.999935\n",
       "2    Angelique Raadgever sterk tegenargument. Ik we...  0.999906\n",
       "3    Sandra Haas toen was het minder duidelijk dat ...  0.999838\n",
       "4    Paula de Gunst mijn vader werd gebeld door de ...  0.999667\n",
       "..                                                 ...       ...\n",
       "228  Vind ik ook. Heb me wel weer laten inenten. Ma...  0.906661\n",
       "229  Joops Eersel QEnergy alleen de antistoffen bli...  0.905045\n",
       "230  Wanneer flikkeren  jullie op met deze dagelijk...  0.904138\n",
       "231  Mariska Berkelaar-Smit heb in een zwaar gerefo...  0.903586\n",
       "232  Ikke wel. Wat een gezever van iedereen. Ben bl...  0.903291\n",
       "\n",
       "[233 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(above_unlabelled_09, old_above, how='inner', on=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3944f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       451\n",
      "           1       0.38      0.49      0.43        49\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.66      0.70      0.68       500\n",
      "weighted avg       0.89      0.87      0.88       500\n",
      "\n",
      "[[412  39]\n",
      " [ 25  24]]\n",
      "[('mijn', 37.5394589326804), ('veranderen', 15.195921665435964), ('voelen', 14.143905054037504), ('moeder', 10.696318957685547), ('gehad', 8.160789016091105), ('en', 7.0674213730451445), ('ouders', 6.775077811674564), ('vader', 6.226580536516842), ('kind', 5.317612302921842), ('de', 4.173522355097137), ('kinderen', 4.117563336583467), ('dochter', 3.9191108606087375), ('mn', 3.5923236220597317), ('heb', 3.2703007162048428), ('ontmaskeren', 3.1096909578192182), ('ook', 3.062119901102135), ('ik', 2.8111040196731327), ('van', 2.7796139972246197), ('ondervinden', 2.6695382279199324), ('te', 2.6206176149749996)]\n",
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       451\n",
      "           1       0.36      0.55      0.44        49\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.66      0.72      0.68       500\n",
      "weighted avg       0.89      0.86      0.87       500\n",
      "\n",
      "[[404  47]\n",
      " [ 22  27]]\n",
      "[('mijn', 0.07606954745757265), ('en', 0.040133188615727136), ('gehad', 0.015218279679826183), ('het', 0.015060751687811774), ('die', 0.01478758952494729), ('voelen', 0.013125200847761283), ('veranderen', 0.011746545175977015), ('ook', 0.010378894536077793), ('de', 0.01019848904808294), ('te', 0.007089895940134645), ('in', 0.0069052894501721186), ('moeder', 0.006366589741507317), ('is', 0.005035260734763819), ('we', 0.004713656978421391), ('heb', 0.004007162248918236), ('ik', 0.0033331273668041977), ('ze', 0.003281975484730183), ('best', 0.0030982258200012262), ('zijn', 0.0030839834713933677), ('man', 0.0028300290842856422)]\n",
      "correct matrix\n",
      "[[ 42  22]\n",
      " [ 27 409]]\n",
      "prediction matrix\n",
      "[[407  30]\n",
      " [ 19  44]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
