{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10166b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from OpenDutchWordnet import Wn_grid_parser\n",
    "import argparse\n",
    "import re\n",
    "import nltk\n",
    "import xgboost\n",
    "import re\n",
    "import numpy as np\n",
    "import stanza\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a207a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        print(text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    no_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove urls\n",
    "    no_urls = re.sub(r\"http\\S+\", \"\", no_emoji)\n",
    "    \n",
    "    # Remove punctuation, numbers and symbols\n",
    "    no_punct_symbols_nrs = re.sub(r'[^A-Za-z\\s]+', '', no_urls)\n",
    "    \n",
    "    # Remove trailing white space\n",
    "    no_trailing_ws = \" \".join(no_punct_symbols_nrs.split())\n",
    "    \n",
    "    # Lowercase\n",
    "    text_clean = no_trailing_ws.lower()\n",
    "    return text_clean\n",
    "\n",
    "def lemmatize(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68445986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(nlp, df_train, df_test):\n",
    "    # Clean text\n",
    "    df_train['clean_text'] = df_train['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "    # Remove empty values\n",
    "    df_train = df_train[df_train['clean_text'] != '']\n",
    "    \n",
    "    # Lemmatize \n",
    "    df_train['lemmatized_clean_text'] = df_train['clean_text'].apply(lambda x: lemmatize(nlp, x))    \n",
    "\n",
    "    df_test['clean_text'] = df_test['message'].apply(lambda x: clean_text(x))\n",
    "    df_test = df_test[df_test['clean_text'] != '']        \n",
    "    df_test['lemmatized_clean_text'] = df_test['clean_text'].apply(lambda x: lemmatize(nlp, x))  \n",
    "    \n",
    "    # Binary labels\n",
    "    df_test['labels'].replace({\"y\": 1, \"n\": 0}, inplace=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b111197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms (instance, synset_id, hypers):\n",
    "    synset = instance.synsets_find_synset(synset_id)\n",
    "    if synset:\n",
    "        hypernyms = synset.get_relations(\"has_hyperonym\")\n",
    "        if hypernyms:\n",
    "            for h in hypernyms:\n",
    "                if (h):\n",
    "                    if not h.get_target() in hypers:\n",
    "                        hypers.append(h.get_target())\n",
    "                        get_hypernyms(instance, h.get_target(), hypers)\n",
    "\n",
    "\n",
    "def get_hypernyms_lemmas():\n",
    "    # ['ontmoeten',\n",
    "    #  'voelen',\n",
    "    #  'meemaken',\n",
    "    #  'ondervinden',\n",
    "    #  'ondergaan',\n",
    "    #  'gevoelen',\n",
    "    #  'zich omkleden',\n",
    "    #  'ervaren',\n",
    "    #  'gewaarworden',\n",
    "    #  'kenteren',\n",
    "    #  'doorleven',\n",
    "    #  'veranderen',\n",
    "    #  'keren',\n",
    "    #  'beleven']\n",
    "    instance = Wn_grid_parser(path_wn_grid_lmf='odwn_orbn_gwg-LMF_1.3.xml.gz')\n",
    "    le_el = instance.les_find_le(\"voelen-v-2\")\n",
    "    synset_el = instance.synsets_find_synset(le_el.get_synset_id())\n",
    "    hypers = []\n",
    "    get_hypernyms(instance, synset_el.get_id(), hypers)\n",
    "    new_hypers = []\n",
    "\n",
    "    lemmas = []\n",
    "    for hyper in hypers:\n",
    "        new_hypers.append(hyper)\n",
    "        for le in instance.les_all_les_of_one_synset(hyper):\n",
    "            lemmas.append(le.get_lemma())  \n",
    "\n",
    "    for hyper in new_hypers:\n",
    "        hypers = []\n",
    "        get_hypernyms(instance, hyper, hypers)\n",
    "        new_hypers = []\n",
    "        for hyper in hypers:\n",
    "            new_hypers.append(hyper)\n",
    "            for le in instance.les_all_les_of_one_synset(hyper):\n",
    "                lemmas.append(le.get_lemma())\n",
    "                \n",
    "    return list(set(lemmas+['voelen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab53939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match(match, clean_text):\n",
    "    if re.search(match, clean_text):\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def WN_lemma_replace(WN_synsets, clean_text, lemmatized_clean_text):\n",
    "    clean_text_tokenized = clean_text.split(\" \")\n",
    "    for wn in WN_synsets:\n",
    "        if wn in lemmatized_clean_text:\n",
    "            for i, word in enumerate(lemmatized_clean_text):\n",
    "                if wn == word:\n",
    "                    clean_text_tokenized[i] = clean_text_tokenized[i].replace(clean_text_tokenized[i], wn)\n",
    "    clean_text = \" \".join(clean_text_tokenized)\n",
    "    return clean_text\n",
    "\n",
    "def remove_matches(match, clean_text, filters):\n",
    "    regex_matches = re.findall(match, clean_text)\n",
    "    if filters == [2]:\n",
    "        removes = regex_matches\n",
    "    else:\n",
    "        removes = list(set((sum(regex_matches, ()))))\n",
    "    return ' '.join([word for word in clean_text.split() if word not in removes])\n",
    "\n",
    "def heuristics_labelling(df_train, df_test, WN_synsets, filters, remove):\n",
    "    \n",
    "    df_train['clean_text_lemma'] = df_train['clean_text']\n",
    "    df_test['clean_text_lemma'] = df_test['clean_text']\n",
    "    \n",
    "    matches = []\n",
    "    if 0 in filters:\n",
    "        matches.append(\"((heb|heeft|hebben) [a-z]* (gehad))\")\n",
    "    if 1 in filters:\n",
    "        matches.append(\"((mijn|mn|me|m n|mij|men) (vader|moeder|ouder|schoonvader|schoonmoeder|kind|zoon|dochter|man|vrouw|broer|zus|neef|nicht|tante|oom))\")\n",
    "    if 2 in filters:\n",
    "        df_train['clean_text_lemma'] = df_train[['clean_text', 'lemmatized_clean_text']].apply(lambda x: WN_lemma_replace(WN_synsets, x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "        df_test['clean_text_lemma'] = df_test[['clean_text', 'lemmatized_clean_text']].apply(lambda x: WN_lemma_replace(WN_synsets, x.clean_text, x.lemmatized_clean_text), axis=1)\n",
    "                \n",
    "        matches.append('('+'|'.join(WN_synsets)+')')\n",
    "\n",
    "    match = '|'.join(matches)\n",
    "    df_train['labels'] = df_train['clean_text_lemma'].apply(lambda x: check_match(match, x))\n",
    "    df_test['predicted'] = df_test['clean_text_lemma'].apply(lambda x: check_match(match, x))\n",
    "      \n",
    "    if remove == True:\n",
    "        df_train['clean_text_removals'] = df_train['clean_text_lemma'].apply(lambda x: remove_matches(match, x, filters))\n",
    "        df_test['clean_text_removals'] = df_test['clean_text_lemma'].apply(lambda x: remove_matches(match, x, filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c9ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df_train, df_train_labels):\n",
    "    over= RandomOverSampler(sampling_strategy=1, random_state=42)\n",
    "    df_train_sampled, df_train_sampled_labels = over.fit_resample(df_train, df_train_labels)\n",
    "    return df_train_sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7baaa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_train, df_test, vectorizer, remove):\n",
    "    vect = vectorizer\n",
    "    y_train = df_train['labels']\n",
    "    y_test = df_test['labels']\n",
    "    if remove == True:\n",
    "        corpus = df_train['clean_text_removals'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text_removals'])\n",
    "    else:\n",
    "        corpus = df_train['clean_text'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text'])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdf3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_unlabelled(df_train, df_test, vectorizer, remove):\n",
    "    vect = vectorizer\n",
    "    y_train = df_train['labels']\n",
    "    if remove == True:\n",
    "        corpus = df_train['clean_text_removals'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text_removals'])\n",
    "    else:\n",
    "        corpus = df_train['clean_text'].tolist()\n",
    "        X_train = vect.fit_transform(corpus)\n",
    "        X_test = vect.transform(df_test['clean_text'])\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78445f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cm(true_labels, predicted_labels):\n",
    "    cm = metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "        \n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                    colorbar=False,\n",
    "                                    show_absolute=False,\n",
    "                                    show_normed=True,\n",
    "                                    class_names=['non-exp','exp'])\n",
    "    fig.set_size_inches(10, 10.5)\n",
    "\n",
    "    \n",
    "def class_feature_importance(X, Y, feature_importances, vect):\n",
    "    N, M = X.shape\n",
    "    X = scale(X, with_mean=False)\n",
    "\n",
    "    out = {}\n",
    "    for c in set(Y):\n",
    "        out[c] = dict(\n",
    "            zip(vect.get_feature_names(), np.mean(X[Y==c, :], axis=0)*feature_importances)\n",
    "        )\n",
    "\n",
    "    return out    \n",
    "\n",
    "def classification_experiments(X_train, y_train, X_test, y_test):\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Logistic Regression\")\n",
    "    \n",
    "    clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"LR: ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "#     create_cm(y_test, y_pred)\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"XGBoost Random Forest\")\n",
    "    \n",
    "    xgbc = XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric='logloss')\n",
    "    xgb = xgbc.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"XGB: ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "#     create_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aefbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_train_test_split(full_df, sub_df, test_df):\n",
    "    test_ids = test_df['object_id'].tolist()\n",
    "    sub_ids = sub_df['object_id'].tolist()\n",
    "    full_df = full_df[~full_df.object_id.isin(test_ids)]\n",
    "    full_df = full_df[~full_df.object_id.isin(sub_ids)]\n",
    "    \n",
    "    # full_df_test is for prediction\n",
    "    full_df_train_no_sentiment, full_df_test = train_test_split(full_df, test_size=0.42, random_state=42)\n",
    "    full_df_train = pd.concat([full_df_train_no_sentiment, sub_df]) #sub_df  is around 2 percent\n",
    "    return full_df_train, full_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea32eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicted(unlabelled_df, unlabelled_X_test, best_model, name):\n",
    "    predictions = best_model.predict_proba(unlabelled_X_test)\n",
    "    unlabelled_df['best_model_pred'] = predictions\n",
    "    unlabelled_df[['text', 'best_model_pred']].to_csv(name+'.tsv', delimiter='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc77b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemmatized_files():\n",
    "    df_train = pd.read_csv('fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.csv', sep='\\t')\n",
    "    df_test = pd.read_csv('experience_test/Fb_random_sample_500_annotated_discussed.tsv', sep='\\t')\n",
    "    sub_df = pd.read_csv('high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.csv', sep='\\t')\n",
    "    \n",
    "    df_train = df_train[~df_train['text'].isna()]\n",
    "    df_test = df_test[~df_test['message'].isna()]\n",
    "    sub_df = sub_df[~sub_df['text'].isna()]\n",
    "    \n",
    "    nlp = stanza.Pipeline(lang='nl', processors='tokenize,pos,lemma')\n",
    "    df_train_sent, df_test_sent = process_data(nlp, sub_df, df_test)\n",
    "    print(\"Finished sentiment\")\n",
    "    df_train, df_test = process_data(nlp, df_train, df_test)\n",
    "    print(\"Finished train and test\")\n",
    " \n",
    "    df_test_sent.to_csv('lemmatized_test_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_train_sent.to_csv('lemmatized_train_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "    df_train.to_csv('lemmatized_train_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t') # no test \n",
    "    df_test.to_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af7ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_models():\n",
    "    # comment this when not using unlabelled    \n",
    "#     df_train = pd.read_csv('lemmatized_train_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "#     sub_df = pd.read_csv('lemmatized_train_high_sent_subjFB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "#     df_train, df_unlabelled = full_data_train_test_split(df_train, sub_df, df_test)\n",
    "#     df_train.to_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t') \n",
    "#     df_unlabelled.to_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "    df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "\n",
    "    WN_synsets = get_hypernyms_lemmas()\n",
    "    \n",
    "    vectorizers = [TfidfVectorizer(), CountVectorizer(binary=True)]\n",
    "    removes = [False, True]\n",
    "    filters = [[0], [1], [2], [0, 1], [1, 2], [0, 2], [0, 1, 2]]\n",
    "    for vect in vectorizers:\n",
    "        print(vect)\n",
    "        print('-----------------')\n",
    "        for remove in removes:\n",
    "            print(remove)\n",
    "            print('-----------------')\n",
    "            for filter_ in filters:\n",
    "                print(filter_)\n",
    "                print('-------------------')\n",
    "                heuristics_labelling(df_train, df_test, WN_synsets, filter_, remove)\n",
    "                print('DATA STATS: ', df_train['labels'].value_counts())\n",
    "\n",
    "                df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "                X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, vect, remove)\n",
    "                print('-------------------')\n",
    "                print(\"BASELINE\")\n",
    "                print(classification_report(df_test['labels'], df_test['predicted']))\n",
    "                classification_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7fe70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_labelling(df_train, df_test, df_unlabelled_test, WN_synsets):\n",
    "    # Best XGB \n",
    "    heuristics_labelling(df_train, df_unlabelled_test, WN_synsets, [0,1,2], False)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    bin_vect = CountVectorizer(binary=True)\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, bin_vect, False)\n",
    "\n",
    "    _, _, unlabelled_X_test = create_train_test_unlabelled(df_train_sampled, df_unlabelled_test, bin_vect, False)\n",
    "    \n",
    "    predictions = best_xgb.predict_proba(unlabelled_X_test)[:, 1]\n",
    "\n",
    "    df_unlabelled_test['best_model_pred'] = predictions\n",
    "    df_unlabelled_test[['text', 'best_model_pred']].to_csv('unlabelled_predictions_by_bestmodel.tsv', sep='\\t', index=False)\n",
    "\n",
    "def best_xgb_results(df_train, df_test, WN_synsets):\n",
    "    heuristics_labelling(df_train, df_test, WN_synsets, [0,1,2], False)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    bin_vect = CountVectorizer(binary=True)\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, bin_vect, False)\n",
    "    best_xgb = XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric='logloss').fit(X_train, y_train)  \n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    importance = best_xgb.feature_importances_\n",
    "\n",
    "    result = class_feature_importance(X_test.toarray(), y_pred, importance, bin_vect)\n",
    "\n",
    "    d = result.get(1)\n",
    "\n",
    "    sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_d[:20])\n",
    "    \n",
    "    return best_xgb, y_pred, y_test\n",
    "    \n",
    "def best_lr_results(df_train, df_test, WN_synsets):\n",
    "    heuristics_labelling(df_train, df_test, WN_synsets, [0,1], True)\n",
    "    df_train_sampled = resample_data(df_train, df_train['labels'])\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train, y_train, X_test, y_test = create_train_test(df_train_sampled, df_test, tfidf, True)\n",
    "    clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Logistic Regression\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    importance = clf.coef_[0]\n",
    "\n",
    "    result = class_feature_importance(X_test.toarray(), y_pred, importance, tfidf)\n",
    "\n",
    "    d = result.get(1)\n",
    "\n",
    "    sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(sorted_d[:20])\n",
    "    \n",
    "    return clf, y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e08c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dfs():\n",
    "    bm_df = pd.read_csv('predicted_data/unlabelled_predictions_by_xgb-binary-keepfeat.tsv', sep='\\t', header=None)\n",
    "#     bm_df[:250].to_csv('predicted_data/top-250-unlabelled_predictions_by_xgb-binary-keepfeat.tsv', sep='\\t', header=False)\n",
    "\n",
    "    bm_df_unlabelled = bm_df.fillna('Unlabelled')\n",
    "    labelled = bm_df_unlabelled[bm_df_unlabelled[3] != 'Unlabelled']\n",
    "    labelled[[0, 1, 3]].to_csv('predicted_data/workshop_labelled_predictions_by_xgb-binary-keepfeat.csv', sep='\\t', header=False)\n",
    "\n",
    "    bm_df_unlabelled = bm_df[~bm_df[1].isin(labelled.index)]\n",
    "    above_unlabelled_09 = bm_df_unlabelled[bm_df_unlabelled[1] >= 0.9]\n",
    "    above_unlabelled_09_sample = above_unlabelled_09.sample(n=250, random_state=50)\n",
    "    above_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_over0-9_by_xgb-binary-keepfeat.csv', sep='\\t', header=False, index=False)\n",
    "    \n",
    "    below_unlabelled_09 = bm_df_unlabelled[bm_df_unlabelled[1] < 0.9]\n",
    "    below_unlabelled_09 = below_unlabelled_09[below_unlabelled_09[1] >= 0.5]\n",
    "    below_unlabelled_09_sample = below_unlabelled_09.sample(n=250, random_state=50)\n",
    "    below_unlabelled_09_sample[[0]].to_csv('predicted_data/sample_unlabelledpredictions_below0-9_by_xgb-binary-keepfeat.csv', sep='\\t', header=False, index=False)\n",
    "\n",
    "    \n",
    "#     sns.histplot(below_250_unlabelled[1], stat=\"probability\", bins=10, kde=True)\n",
    "#     below_250_unlabelled['bin'] = pd.cut(below_250[1], 10)\n",
    "#     print(below_250_unlabelled.bin.value_counts())\n",
    "\n",
    "#     sample_df = below_250_unlabelled.groupby('bin').sample(n=100, random_state=1)\n",
    "#     sample_df[[0, 1, 3, 'bin']].to_csv('predicted_data/sample_unlabelled_predictions_by_xgb-binary-keepfeat.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54f9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sample_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce16349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_preds_index(y_test, y_pred):\n",
    "    return [1 if i == j else 0 for i, j in zip(y_test, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9083be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df_train = pd.read_csv('lemmatized_train_no_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_test = pd.read_csv('lemmatized_test_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    df_unlabelled_test = pd.read_csv('lemmatized_unlabelled_fb_preprocessed_FB_NOS_NU_Telegraaf_NRC_all_endFeb.tsv', sep='\\t')\n",
    "    WN_synsets = get_hypernyms_lemmas()\n",
    "    \n",
    "#     create_lemmatized_files()\n",
    "#     results_models()\n",
    "#     prediction_labelling(df_train, df_test, df_unlabelled_test, WN_synsets)\n",
    "\n",
    "    print(\"LR\")\n",
    "    lr, y_pred_lr, y_test = best_lr_results(df_train, df_test, WN_synsets)\n",
    "    lr_correct = incorrect_preds_index(y_test, y_pred_lr)\n",
    "    \n",
    "    print(\"XGB\")\n",
    "    xgb, y_pred_xgb, y_test = best_xgb_results(df_train, df_test, WN_synsets)\n",
    "    xgb_correct = correct_preds_index(y_test, y_pred_xgb)\n",
    "    create_sample_dfs()\n",
    "\n",
    "    print(\"correct matrix\")\n",
    "    print(confusion_matrix(lr_correct, xgb_correct))\n",
    "    \n",
    "    print(\"prediction matrix\")\n",
    "    print(confusion_matrix(y_pred_lr, y_pred_xgb))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059dd622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:2: DtypeWarning: Columns (24,25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       451\n",
      "           1       0.48      0.43      0.45        49\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.71      0.69      0.70       500\n",
      "weighted avg       0.89      0.90      0.90       500\n",
      "\n",
      "[[428  23]\n",
      " [ 28  21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ouders', 17.501223097464504), ('ik', 13.233149680565687), ('en', 10.550908092880437), ('heeft', 10.305351781605214), ('kinderen', 6.112166758242883), ('in', 6.017148975877178), ('is', 5.656248520819799), ('niet', 5.237692165772117), ('toen', 4.544841165156766), ('ook', 4.369744690592672), ('van', 4.203455138804294), ('was', 3.8428262194430327), ('zei', 3.718128332636853), ('haar', 3.6269143242309183), ('vorig', 3.354185663333869), ('gehad', 3.2988254400502233), ('nooit', 3.2777150919286537), ('geen', 3.2769772580204997), ('op', 3.247193079487749), ('april', 3.1308151499412293)]\n",
      "XGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       451\n",
      "           1       0.39      0.59      0.47        49\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.67      0.74      0.70       500\n",
      "weighted avg       0.90      0.87      0.88       500\n",
      "\n",
      "[[405  46]\n",
      " [ 20  29]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmich\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mijn', 0.09343914668439769), ('en', 0.041772649309964334), ('het', 0.017842216899065472), ('gehad', 0.015569943010862433), ('die', 0.01281091548509963), ('voelen', 0.012730201814351621), ('de', 0.012662943755638402), ('veranderen', 0.010227982211758687), ('te', 0.008897057075766523), ('we', 0.007197630920630591), ('hem', 0.006540817060631899), ('in', 0.005490817695118779), ('moeder', 0.0053881556941996), ('heb', 0.005128675422537871), ('ik', 0.003370459439539001), ('ook', 0.0032107775940498863), ('kind', 0.0028753391959542504), ('ondergaan', 0.0028723613521754745), ('is', 0.0026667395050990876), ('mn', 0.002632448118747101)]\n",
      "(0.00617, 0.105]    41907\n",
      "(0.105, 0.204]       2603\n",
      "(0.204, 0.302]        818\n",
      "(0.302, 0.4]          727\n",
      "(0.4, 0.498]          694\n",
      "(0.498, 0.596]        600\n",
      "(0.891, 0.989]        493\n",
      "(0.596, 0.694]        363\n",
      "(0.694, 0.793]        186\n",
      "(0.793, 0.891]        175\n",
      "Name: bin, dtype: int64\n",
      "correct matrix\n",
      "[[ 20  31]\n",
      " [ 46 403]]\n",
      "prediction matrix\n",
      "[[402  54]\n",
      " [ 23  21]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
